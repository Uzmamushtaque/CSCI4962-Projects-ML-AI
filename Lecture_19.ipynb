{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTVu0FlR+bJTcjHpPzKS1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GRcAUl9lxRQ"
      },
      "source": [
        "# Lecture 19\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. OpenAI Gym ([link](https://gym.openai.com/))\n",
        "2. Multi-Armed Bandit Problem\n",
        "2. Reinforcement Learning with Images\n",
        "3. Important research papers in the field\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VarwOyq8meXG"
      },
      "source": [
        "# Overview\n",
        "\n",
        "1. **gym** python library created by OpenAI\n",
        "2. Allows comparison of different algorithms\n",
        "3. Focused on games\n",
        "4. Standardized Environment\n",
        "\n",
        "\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\"\n",
        "\n",
        "\n",
        "[Source 1](https://openai.com/blog/dall-e/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3EAewN8eQfL"
      },
      "source": [
        "#Example- Frozen lake: Import the Gym Library\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvHFeBfhgI7n",
        "outputId": "cf527e40-e374-41e1-dd82-52016b07c9b8"
      },
      "source": [
        "#go to the website - Create the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()           # put in inital state         \n",
        "env.render()          #Render it"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gpiddB2jfkN",
        "outputId": "16e2b288-9b52-4d98-cb28-5cf13edf0a80"
      },
      "source": [
        "#Inspect Possible Actions\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1sjweXjmGi",
        "outputId": "f2772a4c-eed4-4f4d-a638-73f7c7705340"
      },
      "source": [
        "#create a dummy agent that plays the game randomly\n",
        "MAX_ITERATIONS = 10\n",
        "for i in range(MAX_ITERATIONS):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ13ugrRj6Ng",
        "outputId": "cb42b473-b6dc-4a10-e721-298d65ba2524"
      },
      "source": [
        "import gym\n",
        " \n",
        "actions = {\n",
        "    'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3\n",
        "}\n",
        " \n",
        "print('---- winning sequence ------ ')\n",
        "winning_sequence = (2 * ['Right']) + (3 * ['Down'])+ ['Right']\n",
        "print(winning_sequence)\n",
        " \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break  \n",
        " \n",
        "print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- winning sequence ------ \n",
            "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR5OfL24xfgC"
      },
      "source": [
        "# frozen-lake in deterministic environment\n",
        "#env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "\n",
        "## frozen-lake 8 by 8 version\n",
        "#env = gym.make(\"FrozenLake8x8-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JrwRk15kFlh"
      },
      "source": [
        "[Source](https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SzY9Tgu2Mcd"
      },
      "source": [
        "# DQN on Images\n",
        "\n",
        "Paper 2 (below) is an example of a DQN with images where the agent learns from visual Inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzzZCJ2bybv0"
      },
      "source": [
        "# Multi-armed bandit problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROgatKM0qnKO"
      },
      "source": [
        "# Readings\n",
        "\n",
        "1. [Paper 1](https://arxiv.org/pdf/1908.06973.pdf)\n",
        "2. [Paper 2](https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf)\n",
        "\n",
        "3. [Paper 3](http://proceedings.mlr.press/v119/badia20a/badia20a.pdf)"
      ]
    }
  ]
}