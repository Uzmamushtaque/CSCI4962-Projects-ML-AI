{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP82SakEvOXJyaFgipi0BTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GRcAUl9lxRQ"
      },
      "source": [
        "# Lecture 19\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. OpenAI Gym ([link](https://gym.openai.com/))\n",
        "2. Multi-Armed Bandit Problem\n",
        "2. Reinforcement Learning with Images\n",
        "3. Important research papers in the field\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VarwOyq8meXG"
      },
      "source": [
        "# Overview\n",
        "\n",
        "1. **gym** python library created by OpenAI\n",
        "2. Allows comparison of different algorithms\n",
        "3. Focused on games\n",
        "4. Standardized Environment\n",
        "\n",
        "\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\"\n",
        "\n",
        "\n",
        "[Source 1](https://openai.com/blog/dall-e/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3EAewN8eQfL"
      },
      "source": [
        "#Example- Frozen lake: Import the Gym Library\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvHFeBfhgI7n",
        "outputId": "cf527e40-e374-41e1-dd82-52016b07c9b8"
      },
      "source": [
        "#go to the website - Create the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()           # put in inital state         \n",
        "env.render()          #Render it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gpiddB2jfkN",
        "outputId": "16e2b288-9b52-4d98-cb28-5cf13edf0a80"
      },
      "source": [
        "#Inspect Possible Actions\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1sjweXjmGi",
        "outputId": "f2772a4c-eed4-4f4d-a638-73f7c7705340"
      },
      "source": [
        "#create a dummy agent that plays the game randomly\n",
        "MAX_ITERATIONS = 10\n",
        "for i in range(MAX_ITERATIONS):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ13ugrRj6Ng",
        "outputId": "cb42b473-b6dc-4a10-e721-298d65ba2524"
      },
      "source": [
        "import gym\n",
        " \n",
        "actions = {\n",
        "    'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3\n",
        "}\n",
        " \n",
        "print('---- winning sequence ------ ')\n",
        "winning_sequence = (2 * ['Right']) + (3 * ['Down'])+ ['Right']\n",
        "print(winning_sequence)\n",
        " \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break  \n",
        " \n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- winning sequence ------ \n",
            "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 0.3333333333333333}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR5OfL24xfgC"
      },
      "source": [
        "# frozen-lake in deterministic environment\n",
        "#env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "\n",
        "## frozen-lake 8 by 8 version\n",
        "#env = gym.make(\"FrozenLake8x8-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JrwRk15kFlh"
      },
      "source": [
        "[Source](https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SzY9Tgu2Mcd"
      },
      "source": [
        "# DQN on Images\n",
        "\n",
        "Paper 2 (below) is an example of a DQN with images where the agent learns from visual Inputs.\n",
        "\n",
        "Key ideas :       \n",
        "1. Given visual input, the agent should be able to learn.\n",
        "2. Similar to how humans learn\n",
        "3. Very complex for computers to learn via images.\n",
        "4. Data - For an Atari Breakout game is color important? What resolution should be sufficient to play and learn? The factors that do not impact human performance should not impact agents performance as well\n",
        "5. If agents policy network is learning from some low dimensional data, then for any new game the images should be processed accordingly.\n",
        "6. In the paper below, replay buffer is used to train the agent - Agent policy equals some window of time in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kb8XqkmP5c1"
      },
      "source": [
        "# https://github.com/keras-rl/keras-rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzzZCJ2bybv0"
      },
      "source": [
        "# Multi-armed bandit(MAB) problem\n",
        "\n",
        "The multi-armed bandit problem (sometimes called the K-[1] or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain.\n",
        "\n",
        "Example - imagine that we are faced with k slot machines (one-armed bandits), and we need to figure out which one has the best payout, while not losing too much money.\n",
        "\n",
        "MAB differs from general RL in that actions do not influence the next observation. Another difference is that in Bandits, there are no \"episodes\": every time step starts with a new observation, independently of previous time steps.\n",
        "\n",
        "\n",
        "[Detailed Description](https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc)\n",
        "\n",
        "[Source](https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/bandits_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROgatKM0qnKO"
      },
      "source": [
        "# Readings\n",
        "\n",
        "1. This [Paper 1](https://arxiv.org/pdf/1908.06973.pdf) describes various applications of RL in different domains like computer systems,recommender systems, healthcare systems, finance, Energy Robotics and Transportation.\n",
        "\n",
        "\n",
        "2. [Paper 2](https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf) : In this paper authors develop a novel artificial agent, termed a deep Q-network, that can\n",
        "learn successful policies directly from high-dimensional sensory inputs\n",
        "using end-to-end reinforcement learning. This agent was tested on\n",
        "the challenging domain of classic Atari 2600 games. Authors demonstrate that the deep Q-network agent, receiving only the pixels and\n",
        "the game score as inputs, was able to surpass the performance of all\n",
        "previous algorithms and achieve a level comparable to that of a professional humangamestester across a set of 49 games,using the same\n",
        "algorithm, network architecture and hyperparameters.\n",
        "\n",
        "3. [Paper 3](http://proceedings.mlr.press/v119/badia20a/badia20a.pdf)\n",
        "\n",
        "4. [Paper 4](https://arxiv.org/pdf/1912.07190.pdf)\n",
        "\n",
        "5. [Paper 5](https://dl.acm.org/doi/pdf/10.1145/3219819.3219886)\n",
        "\n",
        "6. [Paper 6](https://arxiv.org/pdf/2108.09141.pdf): Cold strart problem is a difficult problem in online recommendations. Reinforcement-learning researchers apply the principle of actor-critic learning, which is a combination of policy learning and value learning. The policy function plays the role of the actor: it picks what moves to play. The value function is the critic: it tracks whether the agent is ahead or behind in the course of the game. That feedback guides the training process, in the same way that a game review can guide your own study. Inspired by this idea,\n",
        "this paper models the process as a Partially Observable and Controllable\n",
        "Markov Decision Process (POC-MDP), and propose an actor-critic\n",
        "RL framework (RL-LTV) to incorporate the item lifetime values\n",
        "(LTV) into the recommendation. In RL-LTV, the critic studies historical trajectories of items and predict the future LTV of fresh item,\n",
        "while the actor suggests a score-based policy which maximizes the\n",
        "future LTV expectation. Scores suggested by the actor are then\n",
        "combined with classical ranking scores in a dual-rank framework,\n",
        "therefore the recommendation is balanced with the LTV consideration\n",
        "\n",
        "7. [Article](https://www.capestart.com/resources/blog/reinforcement-learning-in-health-care-why-its-important-and-how-it-can-help/) describes the use of RL in healthcare.RL’s most common real-world health care application is the creation and ongoing configuration of DTRs (Dynamic treatment regimes)for patients with longer-term conditions. DTRs are sequences of rules governing health-care decisions – including treatment type, drug dosages, and appointment timing – tailored to an individual patient based on their medical history and conditions over time. Clinical observations and patient assessments provide the input data, with the algorithm outputting treatment options to provide the patient’s most desirable environmental state. RL is used to automate decision-making within these ongoing treatment regimes. It has already helped design DTRs for chronic diseases including cancer and HIV, and could also improve critical care using the rich data collected in intensive care units (ICUs).\n",
        "\n",
        "8.  [Paper 7](https://dl.acm.org/doi/pdf/10.1145/2396761.2396770?casa_token=mjFGDoJ--DsAAAAA:Y5GFp5HzVtFdOULhxpnwgTVFMgz2kaWjI0tenaw_vXhESjIrWJ1_vDrJBviCFa5eE8FMIE3zLTiDeA) "
      ]
    }
  ]
}