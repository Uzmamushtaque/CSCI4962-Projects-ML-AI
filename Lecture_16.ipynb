{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3AtwizaXfIwYAohPvKtFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrKknZfnQTmV"
      },
      "source": [
        "# Lecture 16\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Introduction to Reinforcement learning\n",
        "2. Some Examples\n",
        "3. Elements of Reinforcement Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Reinforcement learning problems involve learning what to do—how to\n",
        "map situations to actions—so as to maximize a numerical reward signal. In\n",
        "an essential way they are closed-loop problems because the learning system’s\n",
        "actions influence its later inputs.\n",
        "\n",
        "Definition: A reward driven trial-and-error process in which a system learns to interact with a complex environment to achieve rewarding outcomes is called *reinforcement learning*. The trial and error process is driven by the need to maximize expected rewards over time. Reinforcement learning can be a gateway to the quest for creating truly intelligent agents such as game playing algorithms, self-driving cars and intelligent robots that interact with the environment.\n",
        "\n",
        "## Reinforcement Learning Involves the following:\n",
        "\n",
        "1. Optimization: Goal is to find an optimal way of making decisions/strategy\n",
        "2. Delayed consequences: Current decisions can impact later outcomes. Challenge is you do not immediately see the outcome of a decision (and long term ramifications).\n",
        "3.  Exploration: Agent is making decisions by learning about the environment. Its only able to get a reward/penalty based on the decision made. Rest of the data is censored. For instance, the agent is unaware what would have happened if they opted for a different option.\n",
        "4. Generalization: A decision policy is to map from past experience and convert that to some action. So why don't we pre-determine some form of rules? That would be an enormous set of rules for complex problems and still not encompass all possibilities so we need some form of generalization.\n",
        "\n",
        "\n",
        "## Difference with other forms of learning\n",
        "\n",
        "1. Supervised learning vs RL: Does not involve Exploration and Delayed Consequence. We are given the experience(dataset). We learn from experience and labels are given.\n",
        "\n",
        "2. Unsupervised vs RL: Does not involve Exploration and Delayed Consequence. No labels provided.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQMTVU8oUO3"
      },
      "source": [
        "# References\n",
        "\n",
        "[Source](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
      ]
    }
  ]
}