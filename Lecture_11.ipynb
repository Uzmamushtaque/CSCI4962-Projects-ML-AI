{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnsTrn8nQriE/COS5JB38J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emdAMv5dEO9t"
      },
      "source": [
        "# Lecture 11\n",
        "\n",
        "# Today's Lecture\n",
        "\n",
        "1. More about CNN\n",
        "2. Case studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXXWpuSkD0J"
      },
      "source": [
        "# Classic Networks\n",
        "\n",
        "#LeNet-5\n",
        "\n",
        "LeNet was used in detecting handwritten cheques by banks based on MNIST dataset. Fully connected networks and activation functions were previously known in neural networks. LeNet-5 introduced convolutional and pooling layers. LeNet-5 is believed to be the base for all other ConvNets.\n",
        "\n",
        "\n",
        "[Source](http://lushuangning.oss-cn-beijing.aliyuncs.com/CNN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/Gradient-Based_Learning_Applied_to_Document_Recognition.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZH3Mtwp9iAa"
      },
      "source": [
        "# AlexNet\n",
        "\n",
        "AlexNet allows for multi-GPU training by putting half of the model's neurons on one GPU and the other half on another GPU. Not only does this mean that a bigger model can be trained, but it also cuts down on the training time. \n",
        "\n",
        "[Source](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7JpqGQ7MJsQ"
      },
      "source": [
        "# VGG-16\n",
        "\n",
        "VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014.\n",
        "\n",
        "[Source](https://arxiv.org/pdf/1409.1556.pdf(2014.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmLmp2c4MXPm"
      },
      "source": [
        "# ResNets\n",
        "\n",
        "ResNet, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of ImageNet challenge in 2015. \n",
        "\n",
        "Major issue posed to most deep networks is that of vanishing/exploding gradients. In image related tasks, there is an added issue of convergence(in a reasonable amount of time) due to complex loss surfaces.\n",
        "\n",
        "Consider an image of a dog standing on a square frame. Some features of this image require fewer layers of learning versus the intricate features that require a deeper network. Convergence will be unnecessarily slow when using a network with fixed depth across all paths to learn concepts (many of these can be learned by shallow networks).\n",
        "\n",
        "The skip connections provide a path of unimpeded gradient flow and therefore has important consequences for backpropagation algorithm. The skip connections create a situation where multiple paths of variable length exist from the input to the output. In such cases, the shortest paths enable the most learning and the longer ones can be viewed as residual contributions. This gives the learning algorithm the flexibility of choosing the appropriate level of non-linearity for a particular input(image). \n",
        "\n",
        "Inputs that an be classified with a small amount of non-linearity will skip many connections. Other inputs with a more complex structure might traverse a large number of connections in order to extract the relevant number of features.\n",
        "\n",
        "The approach is referred to as residual learning, in which learning along longer paths is like fine-tuning of the easier learning along shorter paths. this approach is well suited for tasks where different parts of the same image has different level of complexity.\n",
        "\n",
        "[Source](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp6RGpl3mwD4"
      },
      "source": [
        "# Inception\n",
        "\n",
        "GoogLeNet Architecture of Inception Network:\n",
        "This architecture has 22 layers in total. Using the dimension-reduced inception module, a neural network architecture is constructed. This is popularly known as GoogLeNet (Inception v1).\n",
        " There can be different levels of details that we need for different regions of an image and we do not know what is appropriate for each region upfront.\n",
        " An inception module deals with this issue by performing 3 different convolutions in parallel.\n",
        "\n",
        " Sometimes this may result in computational inefficiency because of large number of convolutions of different sizes. An efficient implementation involves use of 1 by 1 filters. The solution is to use a 1×1 filter to down sample the depth or number of feature maps.\n",
        "\n",
        "A 1×1 filter will only have a single parameter or weight for each channel in the input, and like the application of any filter results in a single output value. This structure allows the 1×1 filter to act like a single neuron with an input from the same position across each of the feature maps in the input. This single neuron can then be applied systematically with a stride of one, left-to-right and top-to-bottom without any need for padding, resulting in a feature map with the same width and height as the input.\n",
        "\n",
        "The 1×1 filter is so simple that it does not involve any neighboring pixels in the input; it may not be considered a convolutional operation. Instead, it is a linear weighting or projection of the input. Further, a nonlinearity is used as with other convolutional layers, allowing the projection to perform non-trivial computation on the input feature maps.\n",
        "\n",
        "[Source](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldVHcrDpr53k"
      },
      "source": [
        "# MobileNets\n",
        "\n",
        "MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used.\n",
        "\n",
        "[Source1](https://arxiv.org/pdf/1704.04861.pdf%EF%BC%89)\n",
        "\n",
        "[Source 2](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0RHDwFUCSlS"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning has the benefit of decreasing the training time for a neural network model and can result in lower generalization error.\n",
        "\n",
        "The weights in re-used layers may be used as the starting point for the training process and adapted in response to the new problem. This usage treats transfer learning as a type of weight initialization scheme. This may be useful when the first related problem has a lot more labeled data than the problem of interest and the similarity in the structure of the problem may be useful in both contexts.\n",
        "\n",
        "[Source](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/)\n",
        "\n",
        "[Data Augmentation Techniques](https://research.aimultiple.com/data-augmentation-techniques/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UUXc2CFnee7"
      },
      "source": [
        "# Introduction to Object Detection (Optional)\n",
        "\n",
        "[Article](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)\n",
        "\n",
        "[Implementation](https://www.tensorflow.org/hub/tutorials/object_detection)\n",
        "\n",
        "\n",
        "[Source](https://arxiv.org/pdf/1311.2524.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QabQHbBNlSyc"
      },
      "source": [
        "# Readings\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/1312.4400.pdf%5c%20http://arxiv.org/abs/1312.4400.pdf)\n",
        "\n"
      ]
    }
  ]
}