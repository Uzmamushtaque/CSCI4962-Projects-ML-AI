{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBs8naO7xA/PTUzlhvLJ2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emdAMv5dEO9t"
      },
      "source": [
        "# Lecture 11\n",
        "\n",
        "# Today's Lecture\n",
        "\n",
        "1. More about CNN\n",
        "2. Case studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXXWpuSkD0J"
      },
      "source": [
        "# Classic Networks\n",
        "\n",
        "#LeNet-5\n",
        "\n",
        "LeNet was used in detecting handwritten cheques by banks based on MNIST dataset. Fully connected networks and activation functions were previously known in neural networks. LeNet-5 introduced convolutional and pooling layers. LeNet-5 is believed to be the base for all other ConvNets.\n",
        "\n",
        "\n",
        "[Source](http://lushuangning.oss-cn-beijing.aliyuncs.com/CNN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97/Gradient-Based_Learning_Applied_to_Document_Recognition.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZH3Mtwp9iAa"
      },
      "source": [
        "# AlexNet\n",
        "\n",
        "AlexNet allows for multi-GPU training by putting half of the model's neurons on one GPU and the other half on another GPU. Not only does this mean that a bigger model can be trained, but it also cuts down on the training time. \n",
        "\n",
        "[Source](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7JpqGQ7MJsQ"
      },
      "source": [
        "# VGG-16\n",
        "\n",
        "VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVR (ImageNet) competition in 2014.\n",
        "\n",
        "[Source](https://arxiv.org/pdf/1409.1556.pdf(2014.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmLmp2c4MXPm"
      },
      "source": [
        "# ResNets\n",
        "\n",
        "ResNet, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of ImageNet challenge in 2015. \n",
        "\n",
        "[Source](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp6RGpl3mwD4"
      },
      "source": [
        "# Inception\n",
        "\n",
        "GoogLeNet Architecture of Inception Network:\n",
        "This architecture has 22 layers in total. Using the dimension-reduced inception module, a neural network architecture is constructed. This is popularly known as GoogLeNet (Inception v1).\n",
        "\n",
        "[Source](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldVHcrDpr53k"
      },
      "source": [
        "# MobileNets\n",
        "\n",
        "MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used.\n",
        "\n",
        "[Source1](https://arxiv.org/pdf/1704.04861.pdf%EF%BC%89)\n",
        "\n",
        "[Source 2](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0RHDwFUCSlS"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning has the benefit of decreasing the training time for a neural network model and can result in lower generalization error.\n",
        "\n",
        "The weights in re-used layers may be used as the starting point for the training process and adapted in response to the new problem. This usage treats transfer learning as a type of weight initialization scheme. This may be useful when the first related problem has a lot more labeled data than the problem of interest and the similarity in the structure of the problem may be useful in both contexts.\n",
        "\n",
        "[Source](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UUXc2CFnee7"
      },
      "source": [
        "# Introduction to Object Detection (Optional)\n",
        "\n",
        "[Article](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)\n",
        "\n",
        "[Implementation](https://www.tensorflow.org/hub/tutorials/object_detection)\n",
        "\n",
        "\n",
        "[Source](https://arxiv.org/pdf/1311.2524.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QabQHbBNlSyc"
      },
      "source": [
        "# Readings\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/1312.4400.pdf%5c%20http://arxiv.org/abs/1312.4400.pdf)\n",
        "\n"
      ]
    }
  ]
}