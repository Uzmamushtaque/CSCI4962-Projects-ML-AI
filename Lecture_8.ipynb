{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPo/I64YIOOEzFW2NnIuWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Had3uxH2oQ"
      },
      "source": [
        "# Lecture 8\n",
        "\n",
        "# Topics for Today\n",
        "\n",
        "1. Word Embeddings\n",
        "2. Learning Word Embeddings\n",
        "3. Applications using word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GNXRT3V8z4i"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "\"*In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.*\" - Source Wikipedia\n",
        "\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "\n",
        "It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
        "\n",
        "Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHhymtLuZLAm"
      },
      "source": [
        "# Visualizing Word Embeddings\n",
        "\n",
        "The new feature space created for words is high dimensional. There are algorithms that help us visualize these embeddings in lower dimensions (dense representations of words in a low-dimensional vector space is usually referred to as word embeddings in most literature):\n",
        "\n",
        "1. t-SNE: t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. In simpler terms, t-SNE gives you an intuition of how the data is arranged in a high-dimensional space. The t-SNE algorithm calculates a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space. It then tries to optimize these two similarity measures using a cost function.\n",
        "\n",
        "2. PCA: PCA is essentially the rotation of coordinate axes , chosen such that each successful axis captures or preserves as much variance as possible. It is the simplest and the most fundamental technique used in dimensionality reduction. PCA is a feature extraction technique of Dimensionality reduction. PCA is an unsupervised technique which doesnâ€™t take the class labels into consideration .\n",
        "\n",
        "\n",
        "[Link](https://projector.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkmBChwy26Fg"
      },
      "source": [
        "# Using Word Embeddings in NLP Applications\n",
        "\n",
        "1. Learn embeddings from large text corpus.\n",
        "\n",
        "2. Transfer embeddings to a new learning task with some training set.\n",
        "\n",
        "3. Continue fine tuning embeddings with new data.\n",
        "\n",
        "# Properties of Word Embeddings\n",
        "\n",
        "Creating analogies using your test set.\n",
        "\n",
        "[Paper 1](https://aclanthology.org/N13-1090.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHIUNSJD9D22"
      },
      "source": [
        "# Learn word embeddings\n",
        "\n",
        "\n",
        "\n",
        "[Paper 2](https://www.jmlr.org/papers/volume3/tmp/bengio03a.pdf)\n",
        "\n",
        "[Paper 3](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
        "\n",
        "[Paper 4](https://aclanthology.org/D14-1162.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1me6--1iOEE"
      },
      "source": [
        "# Lecture Slides\n",
        "\n",
        "[Slides](https://docs.google.com/presentation/d/1J3OoSU6aZU4eScQSrLjTadFWkKMHD4OD/edit?usp=sharing&ouid=106143452455014751731&rtpof=true&sd=true)\n",
        "\n",
        "References:\n",
        "\n",
        "[Article](https://ruder.io/word-embeddings-1/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QK9_dTN1X8_"
      },
      "source": [
        "# Word2vec in Tensorflow\n",
        "\n",
        "[Notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb)"
      ]
    }
  ]
}