{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4FNmZAP6tHV6tmANBLu7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDUThhqyzDhe"
      },
      "source": [
        "# Lecture 6\n",
        "\n",
        "# Topics for Today\n",
        "\n",
        "1. Issues to address when designing a ML/AI project\n",
        "2. Overall Issues/Strategy for a Deep Learning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_Pi0iNCira"
      },
      "source": [
        "# Machine Learning Strategy\n",
        "\n",
        "Based on your first iteration through the model and getting results, you can decide on a number of updates to your models. Some of these can be:\n",
        "\n",
        "1. You want to collect more data\n",
        "2. You want to train your algorithm longer\n",
        "3. Try some optimization algorithm\n",
        "4. Use regularization\n",
        "5. Tune Hyperparameters\n",
        "\n",
        "\n",
        "There are efficient strategies that can help you move in the right direction. We will discuss a few of those in today's lecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kthY9fDeHzl"
      },
      "source": [
        "# Tuning ML Process (Orthogonalization)\n",
        "\n",
        "A typical ML process is:\n",
        "\n",
        "1. Fit training set on the cost function\n",
        "2. Fit dev set on the cost function\n",
        "3. Fit test set\n",
        "\n",
        "If the model performs well on all three steps then we are in an ideal world. Usually this is not the case, therefore we need to fine tune our model at each step. Each step has their own parameters to be tuned. For example if step 1 does not give a good result we may want to focus on getting more data or choosing a good optimization algorithm. In step 2 if we do not get good results, then we might need to use regularization. In step 3 the remedy could be to get a bigger dev set or maybe choose a different model. Thinking of these remedies but according to these 3 different directions is sometimes referred to as orthogonalization. Basically depending on the issue you are facing motivates your remedy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUT1_qybhZ71"
      },
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "When selecting your evaluation metrics there are many aspects that must be taken into consideration.\n",
        "\n",
        "Sometimes selecting two evaluations metrics can be confusing for certain problems. For example Precision and Recall are defined in such a way that sometimes both have a trade-ff. Therefore, for one model you might get a good result for one metric and vice versa. One way to deal with this situation is to come up with a single evaluation metric that would encompass both (or all of your metrics). In this case it could be F1 score which is a harmonic mean of Precision and Recall.\n",
        "\n",
        "Another, way of combining metrics could be to formulate an optimization problem. If model accuracy and runtime are important for your model, then you can constrain your model by solving the maximizing accuracy problem subject to some maximum runtime constraint. This is useful in large neural networks that are achieving high accuracy at the expense of very high runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4NVnlH9qBw_"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "The possible approaches for finding the optimal parameters are:\n",
        "\n",
        "1. Hand tuning (Trial and Error) - This is based on trial and error experiments and experience of the user, parameters are chosen.\n",
        "\n",
        "2. Grid Search - In this a grid is created based on parameter values. And then all possible parameter combinations is tried and and the best one is selected.\n",
        "\n",
        "3. Random Search - In this instead of trying all possible combinations as in Grid Search, only randomly selected subset of the parameters is tried and the best is chosen.\n",
        "\n",
        "4. Bayesian Optimization (Gausian Proces) - Gaussian Process uses a set of previously evaluated parameters and resulting accuracy to make an assumption about unobserved parameters. Acquisition Function using this information suggest the next set of parameters. [link](https://ekamperi.github.io/machine%20learning/2021/05/08/bayesian-optimization.html)\n",
        "\n",
        "\n",
        "5. Tree-structured Parzen Estimators (TPE) - Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. [link](https://medium.com/optuna/multivariate-tpe-makes-optuna-even-more-powerful-63c4bfbaebe2)\n",
        "\n",
        "One important aspect of hyperparameter tuning is that, in most search based methods the logarithms of the hyperparameters are sampled rather than the actual values. For example if searching for $\\eta$ between 0.1 and 0.001 we first sample $log\\eta$ uniformly between -1 and -3 and then exponentiate to the power of 10. However, there are certain parameters that are searched in the uniform space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTwSJ7UZsSEp"
      },
      "source": [
        "# Feature Preprocessing\n",
        "\n",
        "Feature pre-processing in neural networks is not very different from other ML models.\n",
        "\n",
        "1. *Additive Preprocessing and mean centering:* It is useful to mean center the data to remove any kind of bias from the model. Many algorithms like pCA work with the assumption of mean centered data. In practice a vector of column-wise means is subtracted from each data point.\n",
        "\n",
        "A similar type of preprocessing is done to get rid of negative values if it is desired. One way is to add the most negative entry to the rest of the values.\n",
        "\n",
        "2. *Feature Normalization:* A common practice is to divide each feature value by its standard deviation. When this scaling is combines with mean-centering, the data is said to be standardized. The basic idea is that the data is presumed to be drawn from a standard normal distribution with mean zero and unit variance.\n",
        "\n",
        "Another type of feature normalization is to compute the minimum and maximum value of any attribute. Next subtract the min value from the data point and divide it by the difference between max and min.\n",
        "\n",
        "Feature normalization ensures better performance as it is common for the relative values of features to vary more than an order of magnitude. By using these techniques we can lower the sensitivity of the learning algorithm for some features versus the others.\n",
        "\n",
        "3. *Whitening:* Whitening is a technique of creating a new set of de-correlated features. PCA (Principal Component Analysis) is used to achieve this. \n",
        "\n",
        "PCA can be thought of as the application of Singular Value Decomposition (SVD) after mean-centering a data matrix. if D is a $n X m$ mean centered Data matrix and C is a $n X n$ covariance matrix that gives the covariance\n",
        "between dimensions. Therefore, we can say that $C=(D^T D)/n$\n",
        "\n",
        "The eigenvectors of the covariance matrix provide de-correlated directions in the data. Eigenvalues provide variance along each of the directions. Therefore, if we use top-k eigenvectors (largest k eigenvalues) of the covariance matrix, most of the variance in the data will be retained and noise will be removed. One can choose some threshold eigenvalue when selecting these new dimensions.\n",
        "\n",
        "Let the final matrix be P which has dimensions $d X k$, where each column contains one of the top-k eigenvectors. The data matrix D can be transformed into the $k$ dimensional axis system by multiplying with the matrix P. the resulting matrix U will be $n X k$. The rows will contain transformed $n$ points. The variances of the columns of U are the corresponding eigenvalues (because this is the property of the de-correlating transformation of principal component analysis). In whitening each column of U is scaled to unit variance by dividing it with the standard deviation. the transformed features are fed into the neural network. This may change the architecture of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvOllLk5xuUx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsCBPjKhxxQM"
      },
      "source": [
        "# Vanishing and Exploding Gradient Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNvYNFiyW2I"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4QYM0VUah5I"
      },
      "source": [
        "# Readings\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/1907.13359.pdf)"
      ]
    }
  ]
}