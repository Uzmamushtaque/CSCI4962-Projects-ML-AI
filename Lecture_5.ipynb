{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOQighdZayJUx8FpLZRad9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1AYFgLnoDO3"
      },
      "source": [
        "# Lecture 5\n",
        "\n",
        "# Today's Lecture\n",
        "\n",
        "Practical Issues in Neural Network Training:\n",
        "\n",
        "1. Train/Test/Dev sets\n",
        "2. Hyper-parameters\n",
        "3. Problem of Overfitting\n",
        "4. Regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5GO_aWN3tTW"
      },
      "source": [
        "# General\n",
        "\n",
        "Applied machine learning is a highly iterative process. Implementing a simple Neural Network involves numerous configuration steps. Additionally there are multiple input paramters that need to be tuned/initialized. Some of these are (not exhaustive list):\n",
        "1. Number of layers\n",
        "2. Number of Nodes\n",
        "3. Learning Rates\n",
        "4. Activation functions\n",
        "5. Initialization\n",
        "6. Data: Collection and Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3zlWTQ5mg8"
      },
      "source": [
        "# Data Considerations\n",
        "\n",
        "When designing an ML project, we usually begin with an idea, then code it up and then run a few experiemnts to get some results. We may have to go through this cycle multiple times in order to get final results.\n",
        "\n",
        "This iterative process can also involve fitting different configurations of the model to the data until we get the best results. The learning algorithm selects the best model parameters. By refining the model paramters correctly this performance can be improved.\n",
        "\n",
        "The data we are working with, impacts this iterative process and final results as well. The current practice is to follow the following steps when working with your data:\n",
        "\n",
        "1. Split the dataset into train,dev(validation) and test set.\n",
        "2. Train your model on the training set.\n",
        "3. Validate the model on the dev set (also sometimes referred to as the hold-out set).\n",
        "4. Repeat this process to get a good idea of the performance of your model(s) with different model configurations.\n",
        "5. Once we have the best model, we should test the model on the test set to get an unbiased estimate of our final model.\n",
        "\n",
        "The question arises, how to split the data into these 3 sets. A common practice from pre-deep learning era was to split 70-20-10 (train-dev-test).\n",
        "With the advent of Big Data and the fact that deep learning algorithms work well with large amounts of data we can do the split by keeping the following in consideration:\n",
        "\n",
        "1. Size of dev set should be large enough so that we are able to get the best model out of all the options. Using 1000–10000 examples is the ideal number to do validation after training is done. For example for a 1 million sized dataset 10000 dev examples are sufficient.\n",
        "\n",
        "2. Size of test set should be large enough to give you an unbiased estimate of your model performance on unseen data and try to keep your test set around 100–10000 examples for that goal. This is again for a size 1 million dataset.\n",
        "\n",
        "Based on the experimental observations/considerations above we can say that with huge datasets the train-test-dev split is 98-1-1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDkGY3_VIMeq"
      },
      "source": [
        "# Important Considerations\n",
        "\n",
        "1. Your train, dev and test set must come from the same distribution. The data should come from the same distribution as the real data distribution for the problem at hand. If there is a data mismatch then your predictions might not be as accurate as your train-test results.\n",
        "\n",
        "2. Dev set should be large enough to rank the models and test data should be large enough to give you unbiased performance measures of your model.\n",
        "\n",
        "3. You should never make any decision in your iterative learning process after you get the test results. Modeling and fine tuning should be done with the dev set results only. If you make changes to the model after looking at the test results, you are inadvertently breaking the fundamental principle of learning and introducing bias. Test results will therefore no longer be an unbiased estimate of the performance of your model. "
      ]
    }
  ]
}