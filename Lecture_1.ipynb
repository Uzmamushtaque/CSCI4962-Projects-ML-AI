{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAtNEY3348GL87ZU0u023i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7z-3DppCkRr"
      },
      "source": [
        "# Lecture 1\n",
        "## Projects in ML and AI\n",
        "\n",
        "This course is an introduction to some fundamental Machine learning Algorithms in general and Deep Learning in specific.\n",
        "\n",
        "## About the Course\n",
        "\n",
        "1. Syllabus\n",
        "2. Learning outcomes\n",
        "3. Grading\n",
        "4. Emphasis on Practical Applications\n",
        "5. Reading Papers\n",
        "6. Pre-requisites (Algorithms, Linear Algebra, Probability theory)\n",
        "\n",
        "*Sources for the entire Material:*\n",
        "\n",
        "1. *Neural Networks and Deep Learning (Charu C Aggarwal)*\n",
        "2. *Dive Into Deep Learning (Online Edition)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJuMSyR8CKVw"
      },
      "source": [
        "# Logistics\n",
        "\n",
        "Office hours(WebEx): Thursday 1-2 pm or by appointment (email to: mushtu@rpi.edu).\n",
        "\n",
        "WebEx Link: https://rensselaer.webex.com/meet/mushtu\n",
        "\n",
        "Homeworks/Projects will be due on Fridays.\n",
        "\n",
        "Discussion Forum: A new question/discussion will be posted(every 2 weeks) on the Submitty Discussion forum. Participation in the discussion counts towards the particiaption grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I74YmrQtGk6e"
      },
      "source": [
        "# What is Machine Learning?\n",
        "\n",
        "1.   A machine learning algorithm is the process that uncovers the underlying relationship within the data. \n",
        "\n",
        "2.   The outcome of a machine learning algorithm is called machine learning model, which can be considered as a function , which outputs certain results, when given the input.\n",
        "\n",
        "3    Rather than a predefined and fixed function, a machine learning model is derived from historical data. Therefore, when fed with different data, the output of machine learning algorithm changes, i.e. the machine learning model changes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2kTY8E9I4nZ"
      },
      "source": [
        "# Types of Problems in ML and AI\n",
        "\n",
        "•\tSupervised: Regression, Classification\n",
        "\n",
        "•\tUnsupervised\n",
        "\n",
        "•\tReinforcement Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er25kFWhKFZ6"
      },
      "source": [
        "## Artificial Intelligence\n",
        "\n",
        "![picture] (https://drive.google.com/file/d/1TpQXNJ0Qv8yt50Yzt8PoVQUVm9-WcjFn/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnd142KDvvA3"
      },
      "source": [
        "# Today's Lecture\n",
        "\n",
        "1. Resources\n",
        "2. Linear Model\n",
        "3. Regression\n",
        "4. Classification\n",
        "5. Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YsBn3bZwmMb"
      },
      "source": [
        "# Resources for your Projects:\n",
        "\n",
        "•\tJupyter Notebooks\n",
        "\n",
        "•\tGoogle Colab: https://colab.research.google.com/\n",
        "\n",
        "•\tGitHub: Publish your Notebooks on your GitHub. You can directly publish Colab Notebooks on GitHub\n",
        "\n",
        "•\tShare the link to your project as your Homework Submission\n",
        "\n",
        "## Data for your projects\n",
        "\n",
        "1. https://www.kaggle.com/\n",
        "2. https://www.data.gov/\n",
        "3. https://archive.ics.uci.edu/ml/datasets.php\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB3XUFROw6PY"
      },
      "source": [
        "# ML Algorithms’ Grouping\n",
        "\n",
        "*Source for this section: https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms*\n",
        "\n",
        "•\tThe first is a grouping of algorithms by their learning style.\n",
        "\n",
        "•\tThe second is a grouping of algorithms by their similarity in form or function (like grouping similar animals together).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0J7OwKOxT-g"
      },
      "source": [
        "#**Machine Learning Algorithms Grouped by Style**\n",
        "\n",
        "1. **Supervised Learning Algorithms** \n",
        "\n",
        "  Input data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.\n",
        "\n",
        "  A model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.\n",
        "\n",
        "  Example problems are classification and regression.\n",
        "\n",
        "  Example algorithms include: Logistic Regression and the Back Propagation Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiA_AnwXIZoN"
      },
      "source": [
        "2. **Unsupervised Learning Algorithms**\n",
        "\n",
        "  Input data is not labeled and does not have a known result.\n",
        "\n",
        "  A model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.\n",
        "\n",
        "  Example problems are clustering, dimensionality reduction and association rule learning.\n",
        "\n",
        "  Example algorithms include: the Apriori algorithm and K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm4kgW62I5Kd"
      },
      "source": [
        "3. **Semi-Supervised Learning**\n",
        "\n",
        "  Input data is a mixture of labeled and unlabelled examples.\n",
        "\n",
        "  There is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.\n",
        "\n",
        "  Example problems are classification and regression.\n",
        "\n",
        "  Example algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj8dhSSkKMQn"
      },
      "source": [
        "# **Reinforcement Learning**\n",
        "\n",
        "Reinforcement learning is the training of machine learning models to make a sequence of decisions. \n",
        "The agent learns to achieve a goal in an uncertain, potentially complex environment. \n",
        "In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. \n",
        "\n",
        "To get the machine to do what the programmer wants, the artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg4b4QXNJPRa"
      },
      "source": [
        "# **Machine Learning Algorithms Grouped by Similarity**\n",
        "\n",
        "1. **Regression Algorithms**: The most popular regression algorithms are:\n",
        "\n",
        "  Ordinary Least Squares Regression (OLSR)\n",
        "\n",
        "  Linear Regression\n",
        "\n",
        "  Logistic Regression\n",
        "\n",
        "  Stepwise Regression\n",
        "\n",
        "2. **Instance-based Algorithms**\n",
        "    Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model. The most popular instance-based algorithms are:\n",
        "\n",
        "  k-Nearest Neighbor (kNN)\n",
        "\n",
        "  Self-Organizing Map (SOM)\n",
        "\n",
        "  Support Vector Machines (SVM)\n",
        "\n",
        "3. **Regularization Algorithms**\n",
        "\n",
        "  An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.\n",
        "  The most popular regularization algorithms are:\n",
        "\n",
        "  Ridge Regression\n",
        "\n",
        "  Least Absolute Shrinkage and Selection Operator (LASSO)\n",
        "\n",
        "  Elastic Net\n",
        "\n",
        "  Least-Angle Regression (LARS)\n",
        "\n",
        "4. **Decision Tree Algorithms**\n",
        "\n",
        "  Decision tree methods construct a model of decisions made based on actual values of attributes in the data.\n",
        "\n",
        "  The most popular decision tree algorithms are:\n",
        "\n",
        "  Classification and Regression Tree (CART)\n",
        "\n",
        "  Conditional Decision Trees\n",
        "\n",
        "5. **Clustering Algorithms**\n",
        "\n",
        "  Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal.\n",
        "\n",
        "  The most popular clustering algorithms are:\n",
        "\n",
        "  k-Means\n",
        "\n",
        "  k-Medians\n",
        "\n",
        "  Expectation Maximisation (EM)\n",
        "  \n",
        "  Heirarchical Clustering\n",
        "\n",
        "\n",
        "6. **Artificial Neural Network Algorithms**\n",
        "\n",
        "  The most popular artificial neural network algorithms are:\n",
        "\n",
        "  Perceptron\n",
        "\n",
        "  Multilayer Perceptrons (MLP)\n",
        "\n",
        "  Back-Propagation\n",
        "\n",
        "  Stochastic Gradient Descent\n",
        "\n",
        "  Hopfield Network\n",
        "\n",
        "  Radial Basis Function Network (RBFN)\n",
        "\n",
        "7. **Deep Learning Algorithms**\n",
        "\n",
        "  Modern update to Artificial Neural Networks that exploit abundant cheap computation.\n",
        "  The most popular deep learning algorithms are:\n",
        "\n",
        "  Convolutional Neural Network (CNN)\n",
        "\n",
        "  Recurrent Neural Networks (RNNs)\n",
        "\n",
        "  Long Short-Term Memory Networks (LSTMs)\n",
        "\n",
        "  Stacked Auto-Encoders\n",
        "\n",
        "  Deep Boltzmann Machine (DBM)\n",
        "  \n",
        "  Deep Belief Networks (DBN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0T1NSlL1-Fx"
      },
      "source": [
        "# The Linear Model\n",
        "\n",
        "In a linear model, we assume there exists a linear relationship between the indepenedent and dependent variables. We will discuss the linear model from the perspective of Regression (a technique used to predict numeric values mostly).\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "Linear Regression is one of the most basic tools available for regression. We assume that the relationship between the independent variables (denoted by $x$) and the dependent variable (denoted by $y$) is linear. Therefore, $y$ can be expressed as a weighted sum of the elements in $x$.Some noise is permissible on the observations under the assumption that its well-behaved(Gaussian Distribution).\n",
        "\n",
        "**Notation and terms used:** Let us consider an example of predicting House prices based on their area(square foot) and age(in years).\n",
        " In the language of Machine Learning, the dataset is called training data or training set (each row is called an example or datapoint). The value we are trying to predict is the label(target). Within the dataset, the independent variables upon which predictions are based are called features (covariates).\n",
        "\n",
        " Usually we use $n$ to denote the number of examples (observations in our dataset). We index the data examples by $i$ such that each input is \n",
        " $x^{(i)}$ = $[x_1^{(i)},x_2^{(i)}]$ and the corresponding labels as $y^{(i)}$.\n",
        "\n",
        " **The Model**: The linearity assumption states that the target variable(price) can be expressed as a weighted sum of the features:\n",
        "\n",
        "$price = w_{area}.area + w_{age}.age +b$\n",
        "\n",
        "Given a dataset, our goal is to choose the weights  w  and the bias  b  such that on average, the predictions made according to our model best fit the true prices observed in the data. Models whose output prediction is determined by the affine transformation of input features are linear models, where the affine transformation is specified by the chosen weights and bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqoB9q-99YtQ"
      },
      "source": [
        "For a more generic case our inputs consist of  d  features, we express our prediction  $\\hat{y}$  (in general the “hat” symbol denotes estimates) as:\n",
        "\n",
        "$\\hat{y} = w_1x_1 + w_2x_2+...+w_dx_d + b$\n",
        "\n",
        "Collecting features into a vector ${x}$ $\\epsilon$ $R^d$ and all weights into a vector ${w}$ $\\epsilon$ $R^d$, we can represent our model as a dot product:\n",
        "\n",
        " $\\hat{y}$= $\\textbf{w}^Tx + b$\n",
        "\n",
        "The vector  $x$  corresponds to features of a single data example. We will often find it convenient to refer to features of our entire dataset of  $n$  examples via the matrix $\\textbf{X}$ where $\\textbf{X}$ $\\epsilon$ $R^{n\\times d}$ .Here,  $\\textbf{X}$ contains one row for every example and one column for every feature. We can re-write the equation above as (matrix vector product):\n",
        "\n",
        "$\\hat{y}$ = $\\textbf{X}w + b$\n",
        "\n",
        "\n",
        "Before start searching for the best parameters (or model parameters)  $w$  and  $b$ , we will need two more things: \n",
        "\n",
        "(i) a quality measure for some given model; and \n",
        "\n",
        "(ii) a procedure for updating the model to improve its quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQQrrw6dNBA"
      },
      "source": [
        "# Loss Function\n",
        "\n",
        "In any ML problem, we have a target value and a predicted value.The loss function is a way to quantify the difference between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better. The most popular loss function in regression problems is the squared error. \n",
        "\n",
        "Let the predicted value for an example $i$ be $\\hat{y}^{(i)}$ and the corresponding actual label be ${y}^{(i)}$, then the loss function (squared error) is given by:\n",
        "\n",
        "$l^{i}(w,b)$ = $\\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^{2}$\n",
        "\n",
        "The constant  1/2  makes no real difference but will prove notationally convenient, canceling out when we take the derivative of the loss. Since the training dataset is given to us, and thus out of our control, the empirical error is only a function of the model parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv9M9lo_jrI1"
      },
      "source": [
        "To measure the quality of a model on the entire dataset of  $n $ examples, we simply average (or equivalently, sum) the losses on the training set. We now get something known as the cost function:\n",
        "\n",
        "$L(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} l_i(w,b)$\n",
        "\n",
        "When training the model, we want to find parameters ( $w^∗$,$b^∗$ ) that minimize the total loss across all training examples:\n",
        "\n",
        "$w^∗$,$b^∗$ = $argmin_{w,b} \\space L(w,b) $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gpHp0XQk96n"
      },
      "source": [
        "This optimization problem is solved analytically by applying a simple formula. We can assume the bias  $b$  to be a part of  $w$  by appending a column to the design matrix consisting of all ones. Then our prediction problem is to minimize  $∥y−Xw∥^2$ . There is just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. Taking the derivative of the loss with respect to  w  and setting it equal to zero yields the analytic (closed-form) solution:\n",
        "\n",
        "$\\textbf{w}^* = (X^TX)^{-1} X^Ty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcis-awYCkjN"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Binary classification tasks involve predicting categorical outputs. For example Yes/No, Spam/Ham, 0/1 etc. If we use linear regression to predict whetehr an email is spam or not, we will get a continuous outcome. For bounded target values, specifically with a binary outcome logistic regression is used.\n",
        "\n",
        "For predicting a binary outcome we use an activation on top of our linear model(from linear regression). Usually the sigmoid function is used for activation in a bianry classification problem.\n",
        "\n",
        "! [Capture2.PNG](https://drive.google.com/file/d/12WyuqEuA4TApXyIyJyiyvACpIHy8VwYF/view?usp=sharing)\n",
        "\n",
        "The predicted outcome:\n",
        "\n",
        "$\\hat{y}$= $\\sigma(\\textbf{w}^Tx + b)$\n",
        "\n",
        "where $\\sigma(z) = \\frac{1}{(1+e^{-z})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCKFPuR4Lm-m"
      },
      "source": [
        "## Loss Function for Logistic Regression\n",
        "\n",
        "We cannot use the squared error loss function for logistic regression. Can you think why?\n",
        "\n",
        "For a given example $i$, the loss function for a single instance is given by:\n",
        "\n",
        "$l^{i}(y^{(i)},\\hat{y}^{(i)})$ = $-(y^{(i)}\\space log\\hat{y}^{(i)} + (1-y^{i}) log(1-\\hat{y}^{(i)}))$\n",
        "\n",
        "Cost function for the entire data:\n",
        "\n",
        "$L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} l^{i}(y^{(i)},\\hat{y}^{(i)})$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDNajkd-Q8kI"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "In any Machine Learning Algorithm, our goal is to minimize the Cost function. In most cases, the optimization problem cannot be solved analytically. For these cases the technique utilized is that of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the cost function. This algorithm is called **gradient descent**.\n",
        "\n",
        "One basic approach of gradient descent consists of taking the derivative of the cost function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called *minibatch stochastic gradient descent*.\n",
        "\n",
        "Steps to Gradient Descent:\n",
        "1. Initialize the values of the model parameters, typically at random; \n",
        "2. Iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$(w,b) \\leftarrow (w, b)- \\frac{\\eta}{B}\\sum_{i\\epsilon{B}}\\partial_{(w,b)}l^{i}(y^{(i)},\\hat{y}^{(i)})$\n",
        "\n",
        "Here $B$ represents the number of examples in each minibatch (the batch size) and  $\\eta$  denotes the learning rate. The values of the batch size and learning rate are input paramters and not typically learned through model training. \n",
        "\n",
        "These parameters that are tunable but not updated in the training loop are called hyperparameters. Hyperparameter tuning is the process by which hyperparameters are chosen, and typically requires that we adjust them based on the results of the training loop as assessed on a separate validation dataset (or validation set).\n",
        "\n",
        "After training for some predetermined number of iterations (or until some other stopping criteria are met), we record the estimated model parameters, denoted  $\\hat{w},\\hat{b}$ . In most cases these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a finite number of steps.\n",
        "\n",
        "Once we have these learned paramters, we can go back to our problem and calculate the predicted value for each example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8WCwKjKXrlj"
      },
      "source": [
        "## Readings for this week\n",
        "\n",
        "[Paper 1](https://www.researchgate.net/profile/Joanne-Peng-4/publication/242579096_An_Introduction_to_Logistic_Regression_Analysis_and_Reporting/links/0deec5374c228b7fa1000000/An-Introduction-to-Logistic-Regression-Analysis-and-Reporting.pdf)\n",
        "\n",
        "[Paper 2](https://www.southalabama.edu/mathstat/personal_pages/mulekar/BUS622/Steyerberg-2001.pdf)\n",
        "\n",
        "## Next Lecture\n",
        "\n",
        "1. Vectorization\n",
        "2. BroadCasting in Python\n",
        "3. More about Activation Functions\n",
        "4. Pandas, Numpy, Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AmRCwhx7bVY"
      },
      "source": [
        "# Extra Resources\n",
        "\n",
        "[pandas](https://drive.google.com/file/d/1s-yE1CguMH6_2mG4PiAjk8zjp75t8eEm/view?usp=sharing)"
      ]
    }
  ]
}