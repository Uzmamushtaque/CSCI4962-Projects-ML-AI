{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzjJ1QJ2UhTcUr7eknwqdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCAO25F6OMEn"
      },
      "source": [
        "# Lecture 22\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Bias and Fairness in AI\n",
        "2. Algorithmic Bias in AI Systems\n",
        "3. Meaning of Bias\n",
        "4. Ways to mitigate\n",
        "5. Interpretability\n",
        "\n",
        "[Article](https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFxVx6VZjSmV"
      },
      "source": [
        "# The Apple Credit Card Issue\n",
        "\n",
        "Late 2019 saw just how quickly the Apple Card, managed by Goldman Sachs, issue spiralled out of control. What started as a tweet thread with multiple reports of alleged bias (including from Apple’s very own co-founder, Steve Wozniak and his spouse), eventually led to a regulator opening an investigation into Goldman Sachs and their algorithm-prediction practices.\n",
        "\n",
        "Two important issues uncovered by the allegations:\n",
        "\n",
        "1. The algorithm making credit decisions for the Apple card is biased\n",
        "2. The customer support teams from Goldman Sachs and Apple had zero insight into how the algorithm worked when they were asked to explain certain decisions\n",
        "\n",
        "Multiple people had faced a similar outcome where all other input factors being the same (or in some cases higher: like a higher annual income or credit score), and gender being the only difference, they were given significantly lower credit limits than their male spouse.A separate group of non-related women and men ran a test experiment to check for bias and noticed significant differences in credit limits. Men with bad credit scores and irregular income got better offers than women with high incomes and good credit scores. \n",
        "\n",
        "[Source](https://blog.fiddler.ai/2019/11/the-never-ending-issues-around-ai-and-bias-whos-to-blame-when-ai-goes-wrong/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtE0J6Ofminh"
      },
      "source": [
        "# Bias in Computer Vision Algorithms\n",
        "\n",
        "Gender and racial bias in image recognition is introduced when using some state-of-the-art pre-trained models like ImageNet. ([Source](https://arxiv.org/pdf/2010.15052.pdf)) These models  automatically learn racial, gender,\n",
        "and intersectional biases.\n",
        "\n",
        "To explore what kinds of biases may get embedded in image representations generated where class labels aren’t available, the researchers focused on two computer vision models: OpenAI’s iGPT and Google’s SimCLRv2. Both were pretrained on ImageNet 2012, which contains 1.2 million annotated images from Flickr and other photo-sharing sites of 200 object classes. And as the researchers explain, both learn to produce embeddings based on implicit patterns in the entire training set of image features.\n",
        "\n",
        "In addition to racial prejudices, the coauthors report that gender and weight biases exist in the pretrained iGPT and SimCLRv2 models. In a gender-career iEAT test estimating the closeness of the category “male” with “business” and “office” and “female” to attributes like “children” and “home,” embeddings from the models were stereotypical.\n",
        "\n",
        "Both iGPT and SimCLRv2 showed racial prejudices both in terms of valence (i.e., positive and negative emotions) and stereotyping. Embeddings from iGPT and SimCLRv2 exhibited bias for an Arab-Muslim iEAT benchmark measuring whether images of Arab Americans were considered more “pleasant” or “unpleasant” than others. iGPT was biased in a skin tone test comparing perceptions of faces of lighter and darker tones. (Lighter tones were seen by the model to be more “positive.”) And both iGPT and SimCLRv2 associated white people with tools while associating Black people with weapons, a bias similar to that shown by Google Cloud Vision, Google’s computer vision service, which was found to label images of dark-skinned people holding thermometers “gun.”\n",
        "\n",
        "[Source](https://venturebeat.com/2020/11/03/researchers-show-that-computer-vision-algorithms-pretrained-on-imagenet-exhibit-multiple-distressing-biases/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5c7fG7MqEG_"
      },
      "source": [
        "# Bias in Word Embeddings\n",
        "\n",
        "It’s been shown before (‘Man is to computer programmer as woman is to homemaker?’) that word embeddings contain bias. The dominant source of that bias is the input dataset itself, i.e. the text corpus that the embeddings are trained on. The output of any model, learns from what is fed into it. Its highly likely that the results will incorporate the same bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_S0wx75ZkbQ"
      },
      "source": [
        "# Bias and Fairness\n",
        "\n",
        "Bias can exist in many shapes and forms, and can be introduced at any stage in the model development pipeline. At a fundamental level, bias is inherently present in the world around us and encoded into our society. We can broadly define fairness as the absence of prejudice or preference for an individual or group based on their characteristics. \n",
        "\n",
        "# Types of Biases\n",
        "\n",
        "## Data driven\n",
        "1. Selection Bias- Data selection has class imbalance. Does not reflect randomization.\n",
        "2. Sampling Bias - Some data instances are more frequently sampled e.g. based on race, skin color etc.\n",
        "3. Reporting Bias - Data shared does not reflect actual likelihood.\n",
        "\n",
        "## Interpretation driven\n",
        "\n",
        "1. Correlation - Correlation is not equal to causation.\n",
        "2. Overgeneralization - General conclusions drawn from limited data.\n",
        "3. Automation - AI generated decisions favored over human generated decisions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq6t25TzMCme"
      },
      "source": [
        "# Selection Bias\n",
        "\n",
        "Bias can seep into the entire model building process at any stage in the ML pipeline. Selection bias (introduced during data collection stage)is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed.\n",
        "\n",
        "Types of Selection bias:       \n",
        "\n",
        "A. Sampling Bias\n",
        "\n",
        "Examples of sampling bias include \n",
        "\n",
        "1. Self-selection - Non-randomized selection. \n",
        "2. Pre-screening of trial participants, \n",
        "3. Discounting trial subjects/tests that did not run to completion \n",
        "4. Migration bias by excluding subjects who have recently moved into or out of the study area\n",
        "5. Length-time bias, where slowly developing disease with better prognosis is detected\n",
        "\n",
        "\n",
        "B. Time Interval: Early termination of a trial at a time when its results support the desired conclusion.\n",
        "\n",
        "C. Data: Partitioning (dividing) data with knowledge of the contents of the partitions, and then analyzing them with tests designed for blindly chosen partitions.\n",
        "\n",
        "D. Studies: Performing repeated experiments and reporting only the most favorable results, perhaps relabelling lab records of other experiments as \"calibration tests\", \"instrumentation errors\" or \"preliminary surveys\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoP_1bI2uADV"
      },
      "source": [
        "# Class imbalance\n",
        "\n",
        "Skewed data distributions naturally arise in many applications where the positive class occurs with reduced frequency, including data found in disease diagnosis, fraud detection, computer security, and image recognition. \n",
        "\n",
        "Intrinsic imbalance is the result of naturally occurring frequencies of data, e.g. medical diagnoses where the majority of patients are healthy. Extrinsic imbalance, on the other hand, is introduced through external factors, e.g. collection or storage procedures.\n",
        "\n",
        "[Source](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5#Sec2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr3xV8gCDY8D"
      },
      "source": [
        "# Methods to address class imbalance\n",
        "\n",
        "**Data-level methods**\n",
        "\n",
        "Data-level methods for addressing class imbalance include over-sampling and under-sampling. These techniques modify the training distributions in order to decrease the level of imbalance or reduce noise, e.g. mislabeled samples or anomalies. \n",
        "\n",
        "One widely used over-sampling technique is SMOTE(Synthetic Minority Over-sampling Technique).This method produces artificial minority samples by interpolating between existing minority samples and their nearest minority neighbors.\n",
        "\n",
        "Another technique is Batch selection, in which we pick batches of data that have a balnced class ratio. During the learning process the classifier will see a balanced set of data points and produce better quality results.\n",
        "\n",
        "\n",
        "**Algorithm-level methods**\n",
        "\n",
        "Re-weighting: Unlike data sampling methods, algorithmic methods for handling class imbalance do not alter the training data distribution. Instead, the learning or decision process is adjusted in a way that increases the importance of the positive class. Most commonly, algorithms are modified to take a class penalty or weight into consideration, or the decision threshold is shifted in a way that reduces bias towards the negative class.\n",
        "\n",
        "In cost-sensitive learning, penalties are assigned to each class through a cost matrix. Increasing the cost of the minority group is equivalent to increasing its importance, decreasing the likelihood that the learner will incorrectly classify instances from this group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsNaNv5RuCZV"
      },
      "source": [
        "# Bias in features\n",
        "\n",
        "If the classes are balanced even then we can have biases in the model. For instance , the facial recognition problem. There can be latent bias within the data. Due to unequal representation of different racial groups within some popular datasets, biases can seep through the ML pipeline and over/under represent certain groups.\n",
        "\n",
        "Biased facial recognition technology is particularly problematic in law enforcement because errors could lead to false accusations and arrests. \n",
        "\n",
        "[Source](https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZM70oyL8NY"
      },
      "source": [
        "# How to address Bias in ML and AI\n",
        "\n",
        " One important question is if learning pipelines could uncover these biases and help mitigate these. This is an active area of research in Deep Learning and can be divided into 2 broad areas:    \n",
        "\n",
        "1. Bias Mitigation - Given biased model and/or data, can we remove the problematic signal.\n",
        "\n",
        "2. Inclusion - Given data/model, instead of removing signal add back more signal to desired features e.g. underrepresented demographics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI1C1Xl3ujN8"
      },
      "source": [
        "# Bias and Fairness in Supervised Classifier\n",
        "\n",
        "A classifier is biased if its decision changes after being exposed to additional sensitive input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCiPdo8Vu3xD"
      },
      "source": [
        "# Evaluating Bias and Fairness\n",
        "\n",
        "Dissaggregated Evaluation - Evaluate performance w.r.t different sub-groups.\n",
        "\n",
        "Intersectional evaluation - Evaluate performance w.r.t. sub-group intersections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TPo0f95P4tN"
      },
      "source": [
        "# Adversarial Multitask learning to Mitigate Bias\n",
        "\n",
        "The feature to be de-biased is specified and the model predicts the target label and the sensitive feature jointly.\n",
        "\n",
        "An adverserial objective is defined and trained by:     \n",
        "1. Predicting the attribute which is sensitive\n",
        "2. Negate the gradient for that variable in backpropagation\n",
        "3. Remove the effect of z on task decision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y7LQGxGwcuv"
      },
      "source": [
        "# Adaptive Resampling for Automated Debiasing\n",
        "\n",
        "Generative models like VAE (Variational Auto-encoders) can be used to uncover underlying latent variables in a dataset.\n",
        "\n",
        "Once the under-represented and over-represented features are esimated, the training input can adaptively adjust sampling prbabilities such that a fair representative dataset can be used.\n",
        "\n",
        "[Source](https://dl.acm.org/doi/pdf/10.1145/3306618.3314243)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw5Q1rUuDlZE"
      },
      "source": [
        "# Interpretability in AI Models\n",
        "\n",
        "A key property is interpretability — users should have the\n",
        "ability to understand and reason about the model output. In order to have a larger adoption of these models in business and medicine model transparency is needed. \n",
        "\n",
        "Model Transparency: This is defined in terms of three\n",
        "parameters: \n",
        "\n",
        "(i) simulatability – whether a human can\n",
        "use the input data together with the model to reproduce\n",
        "every calculation step necessary to make the prediction.\n",
        "This allows the human to understand the changes in the\n",
        "model parameters caused by the training data; \n",
        "\n",
        "(ii) decomposability – whether there is an intuitive explanation\n",
        "for all the model parameters; and finally \n",
        "\n",
        "(iii) algorithmic\n",
        "transparency – which is essentially an ability to explain\n",
        "the working of the learning algorithm.\n",
        "\n",
        "For instance the choice of a classifier can be explained in a Decision tree in terms of the thresholds and segregation of data points.However, for a deep neural\n",
        "network, the non-linearities added into the features at\n",
        "each layer makes it difficult to explain the features being\n",
        "used for the output.\n",
        "\n",
        "[Source](https://discovery.ucl.ac.uk/id/eprint/10059575/1/Chakraborty_Interpretability%20of%20deep%20learning%20models.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjM452LKDhja"
      },
      "source": [
        "# Readings\n",
        "\n",
        "[paper 1](https://datascience.iq.harvard.edu/files/crcs/files/ai4sg_2020_paper_23.pdf)\n",
        "\n",
        "[paper 2](https://arxiv.org/pdf/1910.12702.pdf)"
      ]
    }
  ]
}