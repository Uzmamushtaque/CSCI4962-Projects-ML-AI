{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjmTkNyjsUdkePP/GXTfhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCAO25F6OMEn"
      },
      "source": [
        "# Lecture 22\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Bias and Fairness in AI\n",
        "2. Algorithmic Bias in AI Systems\n",
        "3. Meaning of Bias\n",
        "4. Ways to mitigate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFxVx6VZjSmV"
      },
      "source": [
        "# The Apple Credit Card Issue\n",
        "\n",
        "Late 2019 saw just how quickly the Apple Card, managed by Goldman Sachs, issue spiralled out of control. What started as a tweet thread with multiple reports of alleged bias (including from Appleâ€™s very own co-founder, Steve Wozniak and his spouse), eventually led to a regulator opening an investigation into Goldman Sachs and their algorithm-prediction practices.\n",
        "\n",
        "Two important issues uncovered by the allegations:\n",
        "\n",
        "1. The algorithm making credit decisions for the Apple card is biased\n",
        "2. The customer support teams from Goldman Sachs and Apple had zero insight into how the algorithm worked when they were asked to explain certain decisions\n",
        "\n",
        "Multiple people had faced a similar outcome where all other input factors being the same (or in some cases higher: like a higher annual income or credit score), and gender being the only difference, they were given significantly lower credit limits than their male spouse.A separate group of non-related women and men ran a test experiment to check for bias and noticed significant differences in credit limits. Men with bad credit scores and irregular income got better offers than women with high incomes and good credit scores. \n",
        "\n",
        "[Source](https://blog.fiddler.ai/2019/11/the-never-ending-issues-around-ai-and-bias-whos-to-blame-when-ai-goes-wrong/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_S0wx75ZkbQ"
      },
      "source": [
        "# Types of Biases\n",
        "\n",
        "## Data driven\n",
        "1. Selection Bias- Data selection has class imbalance. Does not reflect randomization.\n",
        "2. Sampling Bias - Some data instances are more frequently sampled e.g. based on race, skin color etc.\n",
        "3. Reporting Bias - Data shared does not reflect actual likelihood.\n",
        "\n",
        "## Interpretation driven\n",
        "\n",
        "1. Correlation - Correlation is not equal to causation.\n",
        "2. Overgeneralization - General conclusions drawn from limited data.\n",
        "3. Automation - AI generated decisions favored over human generated decisions"
      ]
    }
  ]
}